# AWS Lambda e S3 - Automa√ß√£o de Tarefas

Este reposit√≥rio documenta minha experi√™ncia pr√°tica com AWS Lambda e Amazon S3, implementando automa√ß√£o de tarefas atrav√©s de fun√ß√µes serverless que respondem a eventos do ambiente de armazenamento.

## üìã √çndice

- [Vis√£o Geral do Projeto](#vis√£o-geral-do-projeto)
- [Arquitetura da Solu√ß√£o](#arquitetura-da-solu√ß√£o)
- [Implementa√ß√£o da Lambda Function](#implementa√ß√£o-da-lambda-function)
- [Configura√ß√£o do S3](#configura√ß√£o-do-s3)
- [Anota√ß√µes e Insights](#anota√ß√µes-e-insights)
- [Recursos √öteis](#recursos-√∫teis)

## üéØ Vis√£o Geral do Projeto

Este laborat√≥rio pr√°tico teve como objetivo criar um sistema automatizado onde fun√ß√µes Lambda s√£o acionadas por eventos do Amazon S3, demonstrando o poder da computa√ß√£o serverless para processamento de dados em tempo real.

## üèóÔ∏è Arquitetura da Solu√ß√£o

Implementei uma arquitetura onde:
Upload no S3 ‚Üí Event Notification ‚Üí Lambda Function ‚Üí Processamento ‚Üí A√ß√µes

```

**Fluxo principal:**
1. Arquivo √© enviado para bucket S3
2. S3 dispara notifica√ß√£o para Lambda
3. Lambda executa fun√ß√£o de processamento
4. Resultados s√£o salvos em outro bucket ou banco de dados

## ‚öôÔ∏è Implementa√ß√£o da Lambda Function

### Fun√ß√£o Python para Processamento de Arquivos

```python
import json
import boto3
from datetime import datetime

s3_client = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('FileProcessingLogs')

def lambda_handler(event, context):
    try:
        # Obter informa√ß√µes do evento do S3
        bucket_name = event['Records'][0]['s3']['bucket']['name']
        file_key = event['Records'][0]['s3']['object']['key']
        
        # Log do processamento
        print(f"Processando arquivo: {file_key} do bucket: {bucket_name}")
        
        # Obter metadados do arquivo
        response = s3_client.head_object(Bucket=bucket_name, Key=file_key)
        file_size = response['ContentLength']
        last_modified = response['LastModified']
        
        # Processar conforme o tipo de arquivo
        if file_key.endswith('.csv'):
            process_csv(bucket_name, file_key)
        elif file_key.endswith('.json'):
            process_json(bucket_name, file_key)
        elif file_key.endswith('.txt'):
            process_text(bucket_name, file_key)
        
        # Registrar no DynamoDB
        log_processing(bucket_name, file_key, file_size, 'SUCCESS')
        
        return {
            'statusCode': 200,
            'body': json.dumps('Processamento conclu√≠do com sucesso!')
        }
        
    except Exception as e:
        print(f"Erro no processamento: {str(e)}")
        log_processing(bucket_name, file_key, file_size, 'ERROR', str(e))
        raise e

def process_csv(bucket_name, file_key):
    """Processa arquivos CSV"""
    print(f"Processando CSV: {file_key}")
    # L√≥gica espec√≠fica para CSV
    
def process_json(bucket_name, file_key):
    """Processa arquivos JSON"""
    print(f"Processando JSON: {file_key}")
    # L√≥gica espec√≠fica para JSON

def process_text(bucket_name, file_key):
    """Processa arquivos de texto"""
    print(f"Processando texto: {file_key}")
    # L√≥gica espec√≠fica para texto

def log_processing(bucket_name, file_key, file_size, status, error_message=None):
    """Registra processamento no DynamoDB"""
    table.put_item(
        Item={
            'FileKey': file_key,
            'BucketName': bucket_name,
            'FileSize': file_size,
            'ProcessingTime': datetime.now().isoformat(),
            'Status': status,
            'ErrorMessage': error_message
        }
    )
```

## Configura√ß√£o do IAM Role
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::input-bucket/*",
                "arn:aws:s3:::input-bucket"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:PutObjectAcl"
            ],
            "Resource": "arn:aws:s3:::output-bucket/*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:PutItem",
                "dynamodb:UpdateItem"
            ],
            "Resource": "arn:aws:dynamodb:us-east-1:123456789012:table/FileProcessingLogs"
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
}
```

## Configura√ß√£o do S3

Notifica√ß√£o de Eventos do S3
```
{
    "LambdaFunctionConfigurations": [
        {
            "Id": "ObjectCreatedEvent",
            "LambdaFunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:process-s3-files",
            "Events": ["s3:ObjectCreated:*"],
            "Filter": {
                "Key": {
                    "FilterRules": [
                        {
                            "Name": "suffix",
                            "Value": ".csv"
                        }
                    ]
                }
            }
        },
        {
            "Id": "ObjectCreatedAllEvents",
            "LambdaFunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:process-s3-files",
            "Events": ["s3:ObjectCreated:*"]
        }
    ]
}
```
## Policy do Bucket S3 para Lambda
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "lambda.amazonaws.com"
            },
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::input-bucket/*"
        }
    ]
}
```

## Implementa√ß√£o com AWS CLI
Comandos para Configura√ß√£o
```
# Criar fun√ß√£o Lambda
aws lambda create-function \
    --function-name process-s3-files \
    --runtime python3.9 \
    --role arn:aws:iam::123456789012:role/lambda-s3-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://function.zip

# Configurar notifica√ß√£o do S3
aws s3api put-bucket-notification-configuration \
    --bucket input-bucket \
    --notification-configuration file://notification.json

# Testar a fun√ß√£o Lambda
aws lambda invoke \
    --function-name process-s3-files \
    --payload file://test-event.json \
    output.txt
```

## Exemplo de Evento de Teste
```
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "awsRegion": "us-east-1",
      "eventTime": "2023-01-01T00:00:00.000Z",
      "eventName": "ObjectCreated:Put",
      "s3": {
        "s3SchemaVersion": "1.0",
        "configurationId": "testConfigRule",
        "bucket": {
          "name": "input-bucket",
          "ownerIdentity": {
            "principalId": "EXAMPLE"
          },
          "arn": "arn:aws:s3:::input-bucket"
        },
        "object": {
          "key": "test-file.csv",
          "size": 1024,
          "eTag": "0123456789abcdef0123456789abcdef",
          "versionId": "null"
        }
      }
    }
  ]
}
```

## Anota√ß√µes e Insights

Li√ß√µes Aprendidas:
Vantagens da Arquitetura Serverless: Custo-efetividade: Paga apenas pelo tempo de execu√ß√£o
Escalabilidade autom√°tica: Lida com picos de carga automaticamente
Baixa manuten√ß√£o: Sem gerenciamento de servidores

Padr√µes Comuns de Integra√ß√£o:
S3 ‚Üí Lambda: Processamento de arquivos em tempo real
Lambda ‚Üí DynamoDB: Registro de logs e metadados
Lambda ‚Üí S3: Armazenamento de resultados

Melhores Pr√°ticas Implementadas:
Tratamento robusto de erros
Logging detalhado para debugging
Permiss√µes m√≠nimas necess√°rias no IAM
Timeouts apropriados para diferentes cargas de trabalho

Otimiza√ß√µes de Performance:
Uso de vers√µes compiladas do Python para melhor performance
Conex√µes persistentes com servi√ßos AWS
Processamento em lote quando poss√≠vel

Monitoramento e Troubleshooting:
CloudWatch Logs para monitoramento
M√©tricas customizadas para business metrics
Alertas para falhas de processamento


## Pr√≥ximos Passos
Para aprofundar os conhecimentos em Lambda e S3, planejo:
Implementar processamento ass√≠ncrono com SQS
Explorar Lambda Layers para depend√™ncias compartilhadas
Implementar versionamento e aliases para deployments
Explorar processamento de grandes volumes de dados com Step Functions
Implementar autentica√ß√£o e autoriza√ß√£o com Cognito
